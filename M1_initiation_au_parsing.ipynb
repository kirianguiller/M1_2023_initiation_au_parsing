{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cb8f6c-94df-4e39-b208-e54edc3a9df6",
   "metadata": {},
   "source": [
    "# Initiation au Parsing\n",
    "## Partie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cece58",
   "metadata": {},
   "source": [
    "# Installer le projet\n",
    "/!\\ Faites ceci avant de commencer votre exploration de ce notebook, puisque le notebook a besoin d'avoir certaines d√©pendances d'install√©es\n",
    "\n",
    "\n",
    "Dans votre terminal, lancez \n",
    "```\n",
    "bash install.sh\n",
    "```\n",
    "\n",
    "Ce script :\n",
    "- clone le projet github `stackexchange_dataset`\n",
    "- cr√©e un environnement virtuel python\n",
    "- installe les d√©pendences n√©c√©ssaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## R√©cup√©rer les donn√©es\n",
    "Dans votre terminal, lancez \n",
    "```\n",
    "bash download_data\n",
    "```\n",
    "Ce script va telecharger les forums stackexchange demand√©s. Vous pouvez donc choisir ceux de votre choix. Regardez les commentaires du script pour voir comme modifier la liste de forum √† r√©cup√©rer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√©-code\n",
    "On d√©sactive l'affichage des FutureWarning (librairies, m√©thodes ou arguments qui vont √™tre deprecated dans le futur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pr√©parer les textes\n",
    "Il nous faut d√©zipper les fichiers avant de pouvoir les utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On d√©finit d'abord notre liste de corpus\n",
    "CORPUS_NAMES =  'politics,french,mythology,woodworking,hsm,health,portuguese'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d358411-4001-4000-94e8-65d3b7a6ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dezippage de : politics\n",
      "Dezippage de : french\n",
      "Dezippage de : mythology\n",
      "Dezippage de : woodworking\n",
      "Dezippage de : hsm\n",
      "Dezippage de : health\n",
      "Dezippage de : portuguese\n",
      "Termin√©\n"
     ]
    }
   ],
   "source": [
    "# On import les librairies n√©cessaires pour cette partie\n",
    "import shutil, os\n",
    "\n",
    "# Puis on d√©zippe les fichiers\n",
    "os.makedirs('texts', exist_ok=True)\n",
    "\n",
    "for corpus_name in CORPUS_NAMES:\n",
    "    print(\"Dezippage de :\", corpus_name)\n",
    "    shutil.unpack_archive('./out/'+corpus_name+'.stackexchange.zip', './texts/'+corpus_name)\n",
    "print(\"Termin√©\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809a544-8aa6-4353-9210-c2e3976d4e0e",
   "metadata": {},
   "source": [
    "### Essayont d'analyser un peu nos donn√©es brutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41a2eb62-3309-4636-bca7-8abb87d0a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n"
     ]
    }
   ],
   "source": [
    "corpora = {}\n",
    "corpus_info = []\n",
    "MAX_FILES = 5000\n",
    "for corpus_name in CORPUS_NAMES:\n",
    "    total_text = ''\n",
    "    print(corpus_name)\n",
    "    files_names = os.listdir('texts/'+corpus_name+'/')\n",
    "    for file in files_names[:MAX_FILES]:\n",
    "        total_text += open('texts/'+corpus_name+'/'+file).read()\n",
    "    corpora[corpus_name] = total_text\n",
    "    corpus_info+=[{'corpus':corpus_name, 'nrFiles':len(files_names), 'nrChars':len(total_text)}]\n",
    "print(corpus_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On y voit pas grand chose, essayons d'utiliser pandas, une librairie de \"tableur automatis√©s\" pour comprendre notre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(corpus_info, index=['corpus'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096c2b1-4a5a-415d-94a3-b282c21c80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [corpora[c] for c in corpora]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons les premi√®res lignes de chaque forum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15943e2c-1f46-4681-9837-55b21bf4542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, t in corpora.items():\n",
    "    print('___',c,t[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39022d26",
   "metadata": {},
   "source": [
    "## Faisons quelques graphes de nos corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b75e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x = df.index, y = 'nrFiles', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = df.index, y = 'nrChars', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490446a-700c-4f9a-91cb-2f1377d1e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['charsPerFile']=df['nrChars']/df['nrFiles']\n",
    "sns.barplot(x = df.index, y = 'charsPerFile', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a443910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "typeChars = []\n",
    "for c, t in corpora.items():\n",
    "    counter = collections.Counter(t)\n",
    "    typeChars +=[len(counter)]\n",
    "    print(c,counter)\n",
    "df['typeChars']=typeChars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c9f77-f520-48d0-a36b-14d7d05b3271",
   "metadata": {},
   "source": [
    "## Les Mots et Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31757ef1-e978-40a3-960e-4ba20a3de1a6",
   "metadata": {},
   "source": [
    "### tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment pouvons nous tokeniser la phrase ?\n",
    "text = \"Let's all together defeat last year's problem, SARS-CoV-2, in 2022!\"\n",
    "splitted_text = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758738d9-cd93-43e3-bd78-46f9f034ff79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nochar \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mW+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(nochar\u001b[39m.\u001b[39msplit(text)),\u001b[39mlen\u001b[39m(nochar\u001b[39m.\u001b[39msplit(text))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Avec re, on peut utiliser la fonction split pour tokeniser\n",
    "import re\n",
    "nochar_regex = re.compile('\\W+') # On d√©finit une expression r√©guli√®re pour les caract√®res non-alphanum√©riques\n",
    "'|'.join(nochar_regex.split(text)),len(nochar_regex.split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba307132-73c6-4093-9173-e2e46e9160a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "charorhyphen_regex = re.compile(r'[\\w-]+') # On d√©finit une expression r√©guli√®re pour les caract√®res alphanum√©riques et le tiret\n",
    "'|'.join(charorhyphen_regex.findall(text)),len(charorhyphen_regex.findall(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241bb0b-c965-4318-814f-82dd5c55b88d",
   "metadata": {},
   "source": [
    "### NLTK : word_tokenize\n",
    "On va utiliser la fonction de tokenization de nltk, qui marche correctement pour l'anglais. Si l'on souhaite tokeniser d'autres langues, on peut utiliser les sous modules nltk concern√©s ou bien ceux de spacy.\n",
    "- Pour le chinois, jieba est une bonne solution.\n",
    "- Pour le japonais, mecab.\n",
    "- Pour le thai, pythai-nlp.\n",
    "\n",
    "- Pour les autres langues compliqu√©es sur la tokenisation, il faut faire ses propres recherches avant de se lancer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088ec18-366f-4d3f-97f9-f59dd632519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "'|'.join(word_tokenize(text)),len(word_tokenize(text))\n",
    "# Donc pour tokeniser en fran√ßais : word_tokenize(text, language='french')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3dd883-d6fc-414f-be3b-12b9b856fa37",
   "metadata": {},
   "source": [
    "### Par contre c'est plut√¥t lent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b614ca-1c02-4001-9f70-78acd51ade7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for c, t in tqdm(corpora.items()):\n",
    "    toks = word_tokenize(t[:1000000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4126971-7434-4d97-b2cf-12fe1843e787",
   "metadata": {},
   "source": [
    "Soyez patient pour cette ligne. √áa a pris 44s sur mon pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9011d01-8d97-4fbd-a924-816c8a57baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].map(word_tokenize)\n",
    "df['nrTokens'] = df['tokens'].map(len)\n",
    "df['nrTypes'] = df['tokens'].map(set).map(len)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d1c57-a626-4fac-8e7e-57f001722302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_wordsize'] = df['nrChars']/df['nrTokens']\n",
    "sns.barplot(x = df.index, y = 'avg_wordsize', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46970608-6a40-456d-b530-cf4897cff01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut utiliser la fonction Counter de la librairie collections pour compter les tokens\n",
    "counter = collections.Counter(df['tokens']['mythology'])\n",
    "freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['mythology'])\n",
    "\n",
    "# Quel est le mot le plus fr√©quent dans le corpus mythology ?\n",
    "freq_df.sort_values('mythology',  inplace=True, ascending=False)\n",
    "freq_df.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c147d-9c3c-4366-ad61-9912ea1b70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter = collections.Counter(df['tokens']['mythology'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb0dff-b948-4122-8ffe-4e33f341492b",
   "metadata": {},
   "source": [
    "#### üöß todo:\n",
    "Si vous voulez faire un peu de fouille de donn√©es, vous pouvez r√©pondre aux questions ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f499c3-ffdc-403e-a4c7-c5d23b2af5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average length in the dictionary (on types):',sum(...\n",
    "print('average length in the text (on tokens): ',sum(...\n",
    "print('the longest words: ', sorted(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdebcde-9867-4d04-83ca-ac5e53f4a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove solution:\n",
    "print('average length in the dictionary (on types):',sum([len(t) for t in wordcounter])/len(wordcounter))\n",
    "print('average length in the text (on tokens): ',sum([len(t)*f for t,f in wordcounter.items()])/ sum(wordcounter.values()))\n",
    "print('the longest words: ', sorted(wordcounter, key=lambda x:len(x), reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe322f1-e549-4e5a-8d89-dd5d6208e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenfreq = {} # will contain word length to frequency in the dictionary \n",
    "for t,f in wordcounter.items():\n",
    "    lenfreq[len(t)] = lenfreq.get(len(t),0)+1 # if replacing +1 by + f, we have the length / frequency relation in the texts \n",
    "print(lenfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec8ae7-b27e-486b-8fb6-f530afe497be",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame.from_dict(lenfreq, \n",
    "                                 orient='index', \n",
    "                                 columns=['frequency']).sort_values(by='frequency', ascending=False)\n",
    "                                #\n",
    "# freq_df.sort_index(inplace=True)\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f66a6-764d-4947-86e7-32f0f5631bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df[\"frequency\"].plot(kind='bar', title='check the long tail!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e8197-5af3-485a-99d9-77a034dadb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.head(15).plot(kind='bar', title='without the long tail')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c2592-50e0-4966-95e0-d2ae3166ed80",
   "metadata": {},
   "source": [
    "### different corpus, same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03e7bc-11a8-4884-932d-e1a0569dbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter = collections.Counter(df['tokens']['woodworking'])\n",
    "lenfreq = {} \n",
    "for t,f in wordcounter.items():\n",
    "    lenfreq[len(t)] = lenfreq.get(len(t),0)+1 \n",
    "print(lenfreq)\n",
    "freq_df = pd.DataFrame.from_dict(lenfreq, orient='index', columns=['frequency'])\n",
    "freq_df.sort_index(inplace=True)\n",
    "display(freq_df.head())\n",
    "freq_df.head(15).plot(kind='bar', title='check the long tail!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdc300-0bec-484d-bd8f-b1e7c2eb65d3",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076c0e4-7fef-4c3c-9f20-9da6db790136",
   "metadata": {},
   "source": [
    "## Faisons nos vecteurs de mots\n",
    "On utilise Gensim, une librairie de NLP qui est particulierement facile d'acc√®s pour les plongements vectoriels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a636d56-ad6e-4df3-bae8-a0d546b66c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les hyperparam√®tres du mod√®le\n",
    "Les \"hyperparam√®tres\", en ML, sont les param√®tres d'entrainement d'un mod√®le que l'utilisateur peut modifier pour influer sur la qualit√© du mod√®le. Ce sont ces hyperparam√®tres que l'on va chercher √† optimiser en entrainant plusieurs mod√®les et en selectionnant celui qui performe le mieux sur les fonctions de couts et d'√©valuation\n",
    "\n",
    "Dans notre cas, vous avez plusieurs hyperparam√®tres possibles :\n",
    "- `vector_size` : taille des vecteurs d'embedding \n",
    "- `epochs` : le nombre de fois que le mod√®le s'entrainte sur la totalit√© des donn√©es\n",
    "- `window` : la taille de la fen√™tre de contexte autour du mot √† pr√©dire\n",
    "- `min_count` : nombre d'occurence mini pour un mot pour √™tre dans le vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_models: Dict[str, Word2Vec] = {} # On ajoute du typage statique pour plus de clart√©\n",
    "for c in df.index:\n",
    "    print(c)\n",
    "    embeddings_models[c] = Word2Vec([df['tokens'][c]], min_count=5, epochs=100, vector_size=100, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = {}\n",
    "def get_similarity_scores(embeddings_models, word1, word2):\n",
    "    for model_name in embeddings_models.keys():\n",
    "        model = embeddings_models[model_name]\n",
    "        try :\n",
    "            score = model.wv.similarity(word1, word2)\n",
    "        except KeyError:\n",
    "            score = \"NOT_FOUND\"\n",
    "        similarity_scores[model_name] = score\n",
    "    return similarity_scores\n",
    "\n",
    "def print_similarity_scores(embeddings_models, word1, word2):\n",
    "    print(f\"Computing sim({word1}, {word2})...\")\n",
    "    scores = get_similarity_scores(embeddings_models, word1, word2)\n",
    "    for c, s in scores.items():\n",
    "        print(f\"sim({word1}, {word2}) in {c}: {s}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "print_similarity_scores(embeddings_models, \"sun\", \"moon\")\n",
    "print_similarity_scores(embeddings_models, \"apple\", \"grape\")\n",
    "print_similarity_scores(embeddings_models, \"apple\", \"orange\")\n",
    "print_similarity_scores(embeddings_models, \"nose\", \"mouth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si vous voulez la matrice d'embeddings pour un corpus donn√© :\n",
    "embeddings = model.wv.get_normed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_models[\"mythology\"].wv.most_similar('sun', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pouvez vous savoir pourquoi les r√©sultats sont mauvais ? Essayez de regarder √† nouveau la distribution des mots les plus fr√©quents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# Show all available models in gensim-data\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_25 = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quel va √™tre le mot le plus proche de \"sun\" ?\n",
    "glove_twitter_25.most_similar('sun', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et quel va √™tre le mot le plus proche de \"apple\" ?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mais est-ce que \"apple\" est quand m√™me proche des autres fruits ?\n",
    "glove_twitter_25.similarity('apple', 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a nos vecteurs de mots, on va pouvoir commencer √† les utiliser en entr√©e de nos parseurs. \n",
    "On se garde √ßa pour la semaine prochaine üôå"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
