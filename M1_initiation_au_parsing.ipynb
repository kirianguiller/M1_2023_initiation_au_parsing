{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5cb8f6c-94df-4e39-b208-e54edc3a9df6",
   "metadata": {},
   "source": [
    "# Initiation au Parsing\n",
    "## Partie 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0cece58",
   "metadata": {},
   "source": [
    "# Installer le projet\n",
    "/!\\ Faites ceci avant de commencer votre exploration de ce notebook, puisque le notebook a besoin d'avoir certaines dépendances d'installées\n",
    "\n",
    "\n",
    "Dans votre terminal, lancez \n",
    "```\n",
    "bash install.sh\n",
    "```\n",
    "\n",
    "Ce script :\n",
    "- clone le projet github `stackexchange_dataset`\n",
    "- crée un environnement virtuel python\n",
    "- installe les dépendences nécéssaires"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Récupérer les données\n",
    "Dans votre terminal, lancez \n",
    "```\n",
    "bash download_data\n",
    "```\n",
    "Ce script va telecharger les forums stackexchange demandés. Vous pouvez donc choisir ceux de votre choix. Regardez les commentaires du script pour voir comme modifier la liste de forum à récupérer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pré-code\n",
    "On désactive l'affichage des FutureWarning (librairies, méthodes ou arguments qui vont être deprecated dans le futur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "# Suppress future warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Préparer les textes\n",
    "Il nous faut dézipper les fichiers avant de pouvoir les utiliser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On définit d'abord notre liste de corpus\n",
    "CORPUS_NAMES =  'politics,french,mythology,woodworking,hsm,health,portuguese'.split(',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7d358411-4001-4000-94e8-65d3b7a6ee76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dezippage de : politics\n",
      "Dezippage de : french\n",
      "Dezippage de : mythology\n",
      "Dezippage de : woodworking\n",
      "Dezippage de : hsm\n",
      "Dezippage de : health\n",
      "Dezippage de : portuguese\n",
      "Terminé\n"
     ]
    }
   ],
   "source": [
    "# On import les librairies nécessaires pour cette partie\n",
    "import shutil, os\n",
    "\n",
    "# Puis on dézippe les fichiers\n",
    "os.makedirs('texts', exist_ok=True)\n",
    "\n",
    "for corpus_name in CORPUS_NAMES:\n",
    "    print(\"Dezippage de :\", corpus_name)\n",
    "    shutil.unpack_archive('./out/'+corpus_name+'.stackexchange.zip', './texts/'+corpus_name)\n",
    "print(\"Terminé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d809a544-8aa6-4353-9210-c2e3976d4e0e",
   "metadata": {},
   "source": [
    "### Essayont d'analyser un peu nos données brutes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "41a2eb62-3309-4636-bca7-8abb87d0a053",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "politics\n"
     ]
    }
   ],
   "source": [
    "corpora = {}\n",
    "corpus_info = []\n",
    "MAX_FILES = 5000\n",
    "for corpus_name in CORPUS_NAMES:\n",
    "    total_text = ''\n",
    "    print(corpus_name)\n",
    "    files_names = os.listdir('texts/'+corpus_name+'/')\n",
    "    for file in files_names[:MAX_FILES]:\n",
    "        total_text += open('texts/'+corpus_name+'/'+file).read()\n",
    "    corpora[corpus_name] = total_text\n",
    "    corpus_info+=[{'corpus':corpus_name, 'nrFiles':len(files_names), 'nrChars':len(total_text)}]\n",
    "print(corpus_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On y voit pas grand chose, essayons d'utiliser pandas, une librairie de \"tableur automatisés\" pour comprendre notre dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07ea0b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.DataFrame.from_records(corpus_info, index=['corpus'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9096c2b1-4a5a-415d-94a3-b282c21c80ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['text'] = [corpora[c] for c in corpora]\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualisons les premières lignes de chaque forum "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15943e2c-1f46-4681-9837-55b21bf4542f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, t in corpora.items():\n",
    "    print('___',c,t[:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39022d26",
   "metadata": {},
   "source": [
    "## Faisons quelques graphes de nos corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b75e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.barplot(x = df.index, y = 'nrFiles', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac9eeb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x = df.index, y = 'nrChars', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c490446a-700c-4f9a-91cb-2f1377d1e8da",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['charsPerFile']=df['nrChars']/df['nrFiles']\n",
    "sns.barplot(x = df.index, y = 'charsPerFile', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a443910",
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "typeChars = []\n",
    "for c, t in corpora.items():\n",
    "    counter = collections.Counter(t)\n",
    "    typeChars +=[len(counter)]\n",
    "    print(c,counter)\n",
    "df['typeChars']=typeChars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279c9f77-f520-48d0-a36b-14d7d05b3271",
   "metadata": {},
   "source": [
    "## Les Mots et Tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31757ef1-e978-40a3-960e-4ba20a3de1a6",
   "metadata": {},
   "source": [
    "### tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comment pouvons nous tokeniser la phrase ?\n",
    "text = \"Let's all together defeat last year's problem, SARS-CoV-2, in 2022!\"\n",
    "splitted_text = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758738d9-cd93-43e3-bd78-46f9f034ff79",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb Cell 25\u001b[0m line \u001b[0;36m3\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb#X34sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mre\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb#X34sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m nochar \u001b[39m=\u001b[39m re\u001b[39m.\u001b[39mcompile(\u001b[39m'\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mW+\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/kirian/Projects/courses/M1_syntax_parsing/M1_initiation_au_parsing.ipynb#X34sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m'\u001b[39m\u001b[39m|\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(nochar\u001b[39m.\u001b[39msplit(text)),\u001b[39mlen\u001b[39m(nochar\u001b[39m.\u001b[39msplit(text))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "# Avec re, on peut utiliser la fonction split pour tokeniser\n",
    "import re\n",
    "nochar_regex = re.compile('\\W+') # On définit une expression régulière pour les caractères non-alphanumériques\n",
    "'|'.join(nochar_regex.split(text)),len(nochar_regex.split(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba307132-73c6-4093-9173-e2e46e9160a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "charorhyphen_regex = re.compile(r'[\\w-]+') # On définit une expression régulière pour les caractères alphanumériques et le tiret\n",
    "'|'.join(charorhyphen_regex.findall(text)),len(charorhyphen_regex.findall(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241bb0b-c965-4318-814f-82dd5c55b88d",
   "metadata": {},
   "source": [
    "### NLTK : word_tokenize\n",
    "On va utiliser la fonction de tokenization de nltk, qui marche correctement pour l'anglais. Si l'on souhaite tokeniser d'autres langues, on peut utiliser les sous modules nltk concernés ou bien ceux de spacy.\n",
    "- Pour le chinois, jieba est une bonne solution.\n",
    "- Pour le japonais, mecab.\n",
    "- Pour le thai, pythai-nlp.\n",
    "\n",
    "- Pour les autres langues compliquées sur la tokenisation, il faut faire ses propres recherches avant de se lancer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8088ec18-366f-4d3f-97f9-f59dd632519d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "'|'.join(word_tokenize(text)),len(word_tokenize(text))\n",
    "# Donc pour tokeniser en français : word_tokenize(text, language='french')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d3dd883-d6fc-414f-be3b-12b9b856fa37",
   "metadata": {},
   "source": [
    "### Par contre c'est plutôt lent :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34b614ca-1c02-4001-9f70-78acd51ade7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "for c, t in tqdm(corpora.items()):\n",
    "    toks = word_tokenize(t[:1000000])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4126971-7434-4d97-b2cf-12fe1843e787",
   "metadata": {},
   "source": [
    "Soyez patient pour cette ligne. Ça a pris 44s sur mon pc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9011d01-8d97-4fbd-a924-816c8a57baa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['tokens'] = df['text'].map(word_tokenize)\n",
    "df['nrTokens'] = df['tokens'].map(len)\n",
    "df['nrTypes'] = df['tokens'].map(set).map(len)\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "890d1c57-a626-4fac-8e7e-57f001722302",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['avg_wordsize'] = df['nrChars']/df['nrTokens']\n",
    "sns.barplot(x = df.index, y = 'avg_wordsize', data = df, palette='husl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46970608-6a40-456d-b530-cf4897cff01f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut utiliser la fonction Counter de la librairie collections pour compter les tokens\n",
    "counter = collections.Counter(df['tokens']['mythology'])\n",
    "freq_df = pd.DataFrame.from_dict(counter, orient='index', columns=['mythology'])\n",
    "\n",
    "# Quel est le mot le plus fréquent dans le corpus mythology ?\n",
    "freq_df.sort_values('mythology',  inplace=True, ascending=False)\n",
    "freq_df.head(22)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c5c147d-9c3c-4366-ad61-9912ea1b70c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter = collections.Counter(df['tokens']['mythology'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53fb0dff-b948-4122-8ffe-4e33f341492b",
   "metadata": {},
   "source": [
    "#### 🚧 todo:\n",
    "Si vous voulez faire un peu de fouille de données, vous pouvez répondre aux questions ci-dessous :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f499c3-ffdc-403e-a4c7-c5d23b2af5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('average length in the dictionary (on types):',sum(...\n",
    "print('average length in the text (on tokens): ',sum(...\n",
    "print('the longest words: ', sorted(..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdebcde-9867-4d04-83ca-ac5e53f4a1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove solution:\n",
    "print('average length in the dictionary (on types):',sum([len(t) for t in wordcounter])/len(wordcounter))\n",
    "print('average length in the text (on tokens): ',sum([len(t)*f for t,f in wordcounter.items()])/ sum(wordcounter.values()))\n",
    "print('the longest words: ', sorted(wordcounter, key=lambda x:len(x), reverse=True)[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe322f1-e549-4e5a-8d89-dd5d6208e1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "lenfreq = {} # will contain word length to frequency in the dictionary \n",
    "for t,f in wordcounter.items():\n",
    "    lenfreq[len(t)] = lenfreq.get(len(t),0)+1 # if replacing +1 by + f, we have the length / frequency relation in the texts \n",
    "print(lenfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eec8ae7-b27e-486b-8fb6-f530afe497be",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df = pd.DataFrame.from_dict(lenfreq, \n",
    "                                 orient='index', \n",
    "                                 columns=['frequency']).sort_values(by='frequency', ascending=False)\n",
    "                                #\n",
    "# freq_df.sort_index(inplace=True)\n",
    "freq_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2f66a6-764d-4947-86e7-32f0f5631bb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df[\"frequency\"].plot(kind='bar', title='check the long tail!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e8197-5af3-485a-99d9-77a034dadb44",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_df.head(15).plot(kind='bar', title='without the long tail')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c2592-50e0-4966-95e0-d2ae3166ed80",
   "metadata": {},
   "source": [
    "### different corpus, same graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03e7bc-11a8-4884-932d-e1a0569dbee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcounter = collections.Counter(df['tokens']['woodworking'])\n",
    "lenfreq = {} \n",
    "for t,f in wordcounter.items():\n",
    "    lenfreq[len(t)] = lenfreq.get(len(t),0)+1 \n",
    "print(lenfreq)\n",
    "freq_df = pd.DataFrame.from_dict(lenfreq, orient='index', columns=['frequency'])\n",
    "freq_df.sort_index(inplace=True)\n",
    "display(freq_df.head())\n",
    "freq_df.head(15).plot(kind='bar', title='check the long tail!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdc300-0bec-484d-bd8f-b1e7c2eb65d3",
   "metadata": {},
   "source": [
    "-----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8076c0e4-7fef-4c3c-9f20-9da6db790136",
   "metadata": {},
   "source": [
    "## Faisons nos vecteurs de mots\n",
    "On utilise Gensim, une librairie de NLP qui est particulierement facile d'accès pour les plongements vectoriels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a636d56-ad6e-4df3-bae8-a0d546b66c9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "from typing import Dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Les hyperparamètres du modèle\n",
    "Les \"hyperparamètres\", en ML, sont les paramètres d'entrainement d'un modèle que l'utilisateur peut modifier pour influer sur la qualité du modèle. Ce sont ces hyperparamètres que l'on va chercher à optimiser en entrainant plusieurs modèles et en selectionnant celui qui performe le mieux sur les fonctions de couts et d'évaluation\n",
    "\n",
    "Dans notre cas, vous avez plusieurs hyperparamètres possibles :\n",
    "- `vector_size` : taille des vecteurs d'embedding \n",
    "- `epochs` : le nombre de fois que le modèle s'entrainte sur la totalité des données\n",
    "- `window` : la taille de la fenêtre de contexte autour du mot à prédire\n",
    "- `min_count` : nombre d'occurence mini pour un mot pour être dans le vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_models: Dict[str, Word2Vec] = {} # On ajoute du typage statique pour plus de clarté\n",
    "for c in df.index:\n",
    "    print(c)\n",
    "    embeddings_models[c] = Word2Vec([df['tokens'][c]], min_count=5, epochs=100, vector_size=100, window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "similarity_scores = {}\n",
    "def get_similarity_scores(embeddings_models, word1, word2):\n",
    "    for model_name in embeddings_models.keys():\n",
    "        model = embeddings_models[model_name]\n",
    "        try :\n",
    "            score = model.wv.similarity(word1, word2)\n",
    "        except KeyError:\n",
    "            score = \"NOT_FOUND\"\n",
    "        similarity_scores[model_name] = score\n",
    "    return similarity_scores\n",
    "\n",
    "def print_similarity_scores(embeddings_models, word1, word2):\n",
    "    print(f\"Computing sim({word1}, {word2})...\")\n",
    "    scores = get_similarity_scores(embeddings_models, word1, word2)\n",
    "    for c, s in scores.items():\n",
    "        print(f\"sim({word1}, {word2}) in {c}: {s}\")\n",
    "    print()\n",
    "    \n",
    "\n",
    "print_similarity_scores(embeddings_models, \"sun\", \"moon\")\n",
    "print_similarity_scores(embeddings_models, \"apple\", \"grape\")\n",
    "print_similarity_scores(embeddings_models, \"apple\", \"orange\")\n",
    "print_similarity_scores(embeddings_models, \"nose\", \"mouth\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Si vous voulez la matrice d'embeddings pour un corpus donné :\n",
    "embeddings = model.wv.get_normed_vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_models[\"mythology\"].wv.most_similar('sun', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pouvez vous savoir pourquoi les résultats sont mauvais ? Essayez de regarder à nouveau la distribution des mots les plus fréquents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "# Show all available models in gensim-data\n",
    "\n",
    "print(list(gensim.downloader.info()['models'].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_twitter_25 = gensim.downloader.load('glove-twitter-25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quel va être le mot le plus proche de \"sun\" ?\n",
    "glove_twitter_25.most_similar('sun', topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Et quel va être le mot le plus proche de \"apple\" ?\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mais est-ce que \"apple\" est quand même proche des autres fruits ?\n",
    "glove_twitter_25.similarity('apple', 'orange')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maintenant que l'on a nos vecteurs de mots, on va pouvoir commencer à les utiliser en entrée de nos parseurs. \n",
    "On se garde ça pour la semaine prochaine 🙌"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
